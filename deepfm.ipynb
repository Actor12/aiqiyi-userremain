{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.doubanio.com/simple/, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Collecting deepctr[gpu]\n",
      "  Downloading https://pypi.doubanio.com/packages/e6/7e/e39d5aa9fd6e511e85c779d0d67e3be8c1b7f2982ccabf2691eed1c3000f/deepctr-0.8.7-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from deepctr[gpu]) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from deepctr[gpu]) (2.10.0)\n",
      "Collecting tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"\n",
      "  Using cached https://pypi.doubanio.com/packages/b8/f3/9c15ce0b689076ac937b4d6c1d9ee8beea7b0a2f7aeac8aaba1377be3845/tensorflow_gpu-2.5.0-cp36-cp36m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deepctr[gpu]) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py==2.10.0->deepctr[gpu]) (1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py==2.10.0->deepctr[gpu]) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.7.4.1)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading https://pypi.doubanio.com/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.11.3)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading https://pypi.doubanio.com/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.5\n",
      "  Downloading https://pypi.doubanio.com/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 55.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading https://pypi.doubanio.com/packages/23/47/835652c7e19530973c73c65e652fc53bd05725d5a7cf9bb8706777869c1e/absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 56.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n",
      "  Downloading https://pypi.doubanio.com/packages/aa/e7/53bc896aa4e11a87aac10a625c676b3a3d57d1c8d9929e4809d31fa0b7d5/keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 56.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.34.0\n",
      "  Using cached https://pypi.doubanio.com/packages/ab/f5/3d3bcb82beae990021cbf6877456a1aab650e68c902194566edd6a73e37c/grpcio-1.34.1-cp36-cp36m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.12.1)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading https://pypi.doubanio.com/packages/ec/78/b27f73e923becc6e79e18fe112cf75e3200d1ee35b0dba8fa46181bce56c/tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 53.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading https://pypi.doubanio.com/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.2.0)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading https://pypi.doubanio.com/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading https://pypi.doubanio.com/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 5.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading https://pypi.doubanio.com/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (46.0.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.0.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading https://pypi.doubanio.com/packages/1a/c1/499e600ba0c618b451cd9c425ae1c177249940a2086316552fee7d86c954/tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 66.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.11.3)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.4.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading https://pypi.doubanio.com/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 69.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu!=1.7.*,!=1.8.*,>=1.4.0; extra == \"gpu\"->deepctr[gpu]) (3.1.0)\n",
      "\u001b[31mERROR: tensorflow 2.1.0 has requirement gast==0.2.2, but you'll have gast 0.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.1.0 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 2.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.1.0 has requirement tensorflow-estimator<2.2.0,>=2.1.0rc0, but you'll have tensorflow-estimator 2.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.18.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-gpu 2.5.0 has requirement six~=1.15.0, but you'll have six 1.14.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: flatbuffers, keras-preprocessing, tensorboard-plugin-wit, wheel, absl-py, grpcio, tensorboard-data-server, tensorboard, keras-nightly, tensorflow-estimator, gast, opt-einsum, astunparse, tensorflow-gpu, deepctr\n",
      "\u001b[33m  WARNING: The script wheel is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 deepctr-0.8.7 flatbuffers-1.12 gast-0.4.0 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 opt-einsum-3.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-estimator-2.5.0 tensorflow-gpu-2.5.0 wheel-0.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install deepctr[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score,f1_score\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr.models import DeepFM,xDeepFM ,FGCNN, NFM#NFFM,\n",
    "#from deepctr.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.feature_column import SparseFeat,DenseFeat,get_feature_names\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/new_data/train_data.txt',sep='\\t')\n",
    "test = pd.read_csv('./data/new_data/test_data.txt',sep='\\t')\n",
    "#data = pd.concat([train, test])\n",
    "train[\"launch_seq\"] = train.launch_seq.apply(lambda x: json.loads(x))\n",
    "train[\"playtime_seq\"] = train.playtime_seq.apply(lambda x: json.loads(x))\n",
    "train[\"duration_prefer\"] = train.duration_prefer.apply(lambda x: json.loads(x))\n",
    "train[\"interact_prefer\"] = train.interact_prefer.apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tagid'] = data['tagid'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = data['tagid'].values.tolist()\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [str(x) for x in sentences[i]]   #将每个tagid转化成str格式\n",
    "\n",
    "emb_size = 128\n",
    "#model = Word2Vec(sentences,vector_size=emb_size, window=10, min_count=1, sg=0, hs=0, seed=1,epochs=5)\n",
    "model = Word2Vec.load('./w2vmodel/w2vmodel.model')\n",
    "emb_matrix = []\n",
    "for seq in sentences:\n",
    "    vec = []\n",
    "    for w in seq:\n",
    "#         if w in model.wv.vocab:\n",
    "#             vec.append(model.wv[w])\n",
    "        try:\n",
    "            vec.append(model.wv[w])\n",
    "        except KeyError:\n",
    "                continue\n",
    "                \n",
    "    if len(vec) > 0:\n",
    "        emb_matrix.append(np.mean(vec, axis=0))\n",
    "    else:\n",
    "        emb_matrix.append([0] * emb_size)\n",
    "emb_matrix = np.array(emb_matrix)\n",
    "for i in range(emb_size):\n",
    "    data['tag_emb_{}'.format(i)] = emb_matrix[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#增加model特征\n",
    "def replace_dup(data):\n",
    "    data = data.replace('vivo',\"VIVO\")\n",
    "    data = data.replace('Moto',\"摩托罗拉\")\n",
    "    data = data.replace('索尼移动',\"索尼\")\n",
    "    data = data.replace('移动',\"中国移动\")\n",
    "    data = data.replace('8848钛金手机',\"8848\")\n",
    "    \n",
    "    return data\n",
    "data['make'] = replace_dup(data['make'])\n",
    "\n",
    "vs = ['华为', '魅族', 'OPPO', '360', '荣耀', '小米', '联想', '努比亚', 'VIVO',\n",
    "        '金立', '邦华', '锤子科技', '乐视', '海信', '一加', '苹果', '美图', '中国移动',\n",
    "          '中兴', '魅果', '酷比', '小辣椒', '国美', '小天才', '酷派',\n",
    "      '优博讯', '8848', '虹动', '康佳', '索尼', 'COMIO', '朵唯', '进化者',\n",
    "         '夏普', '长虹', '格力', 'ALLWINNER', '创维', '黑莓', '好爱', '诺亚信',\n",
    "        '天语', '三星', '贝尔丰', 'SUGAR', '极豆DVD导航', '读书郎',\n",
    "         '众赢时代', 'AGM', 'ivvi', '赛博宇华', '米语',\n",
    "        '东方拓宇',  'alps', '欧亚信', 'vetas', '传奇']\n",
    "vs1 = ['华为', '魅族', 'OPPO', '360', '荣耀', '小米', '联想', '努比亚', 'VIVO',\n",
    "        '金立', '邦华', '锤子科技', '乐视', '海信', '一加', '苹果', '美图', '中国移动',\n",
    "          '中兴','酷比', '小辣椒', '小天才', '酷派']\n",
    "data['make'] = data['make'].apply(lambda x:x if x in vs1 else \"未知\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tagidLen'] = data['tagid'].apply(lambda x:len(x))\n",
    "data['age'] = data['age'].fillna(0)\n",
    "data['gender'] = data['gender'].fillna(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pid', 'label', 'gender', 'age', 'tagid', 'time', 'province', 'city',\n",
       "       'make', 'model',\n",
       "       ...\n",
       "       'tag_emb_119', 'tag_emb_120', 'tag_emb_121', 'tag_emb_122',\n",
       "       'tag_emb_123', 'tag_emb_124', 'tag_emb_125', 'tag_emb_126',\n",
       "       'tag_emb_127', 'tagidLen'],\n",
       "      dtype='object', length=139)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [i for i in data.columns if i not in ['pid', 'tagid', 'time', 'model','city']]  ##尝试加入用户pid，过拟合\n",
    "data = data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = ['province', 'make']\n",
    "dense_features = ['gender', 'age','tagidLen']\n",
    "emb_feat = ['tag_emb_{}'.format(i) for i in range(emb_size)]\n",
    "# for i in data.columns:\n",
    "#     if data[i].nunique() > 1000:\n",
    "#         dense_features.append(i)\n",
    "#     else:\n",
    "#         if i == \"label\":\n",
    "#             break\n",
    "#         sparse_features.append(i)\n",
    "\n",
    "# data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "# data[dense_features] = data[dense_features].fillna(0, )\n",
    "# target = ['label']\n",
    "\n",
    "# # 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "#fm层的类别数据要先编码，然后会有个embedding操作\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "#dense的连续数据标准化\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "#print(data.head())\n",
    "# # 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "#深层特征列\n",
    "#将稀疏特征进一步通过嵌入技术将其转成稠密向量，将稠密特征拼接到全连接神经网络的输入向量\n",
    "#标签编码\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=\"auto\")\n",
    "                       for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,)\n",
    "                      for feat in dense_features] \n",
    "#哈希编码\n",
    "# fixlen_feature_columns = [SparseFeat(feat,vocabulary_size=data[feat].nunique(),embedding_dim=4,use_hash=True)\n",
    "#                         for feat in sparse_features] + [DenseFeat(feat, 1, )\n",
    "#                       for feat in dense_features]\n",
    "\n",
    "#加入embeding特征\n",
    "fixlen_feature_columns.extend([DenseFeat(i, 1) for i in emb_feat])\n",
    "\n",
    "#生成特征列\n",
    "dnn_feature_columns = fixlen_feature_columns  #用做DNN模型的输入向量\n",
    "linear_feature_columns = fixlen_feature_columns  #用做线性模型的输入特征\n",
    "\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)#获取所有特征名\n",
    "\n",
    "#all_feature = feature_names + emb_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>make</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tagidLen</th>\n",
       "      <th>tag_emb_0</th>\n",
       "      <th>tag_emb_1</th>\n",
       "      <th>tag_emb_2</th>\n",
       "      <th>tag_emb_3</th>\n",
       "      <th>tag_emb_4</th>\n",
       "      <th>...</th>\n",
       "      <th>tag_emb_118</th>\n",
       "      <th>tag_emb_119</th>\n",
       "      <th>tag_emb_120</th>\n",
       "      <th>tag_emb_121</th>\n",
       "      <th>tag_emb_122</th>\n",
       "      <th>tag_emb_123</th>\n",
       "      <th>tag_emb_124</th>\n",
       "      <th>tag_emb_125</th>\n",
       "      <th>tag_emb_126</th>\n",
       "      <th>tag_emb_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079127</td>\n",
       "      <td>-0.584356</td>\n",
       "      <td>-0.505823</td>\n",
       "      <td>0.163237</td>\n",
       "      <td>-0.118881</td>\n",
       "      <td>-0.570097</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199300</td>\n",
       "      <td>-0.039976</td>\n",
       "      <td>0.120165</td>\n",
       "      <td>0.704987</td>\n",
       "      <td>0.455543</td>\n",
       "      <td>-0.556336</td>\n",
       "      <td>0.340716</td>\n",
       "      <td>-0.544948</td>\n",
       "      <td>-0.441241</td>\n",
       "      <td>-0.371886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.083220</td>\n",
       "      <td>-1.238055</td>\n",
       "      <td>-0.382685</td>\n",
       "      <td>-0.172527</td>\n",
       "      <td>-1.336101</td>\n",
       "      <td>-0.141902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149191</td>\n",
       "      <td>0.512553</td>\n",
       "      <td>-0.193722</td>\n",
       "      <td>0.661562</td>\n",
       "      <td>-0.182866</td>\n",
       "      <td>-0.928568</td>\n",
       "      <td>-0.151268</td>\n",
       "      <td>0.027404</td>\n",
       "      <td>0.356565</td>\n",
       "      <td>0.194155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065484</td>\n",
       "      <td>-0.207355</td>\n",
       "      <td>-0.698800</td>\n",
       "      <td>-0.361576</td>\n",
       "      <td>-0.397024</td>\n",
       "      <td>-0.512204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337251</td>\n",
       "      <td>-1.367355</td>\n",
       "      <td>-0.110100</td>\n",
       "      <td>0.931193</td>\n",
       "      <td>-0.368334</td>\n",
       "      <td>-0.824169</td>\n",
       "      <td>-0.105975</td>\n",
       "      <td>-0.491890</td>\n",
       "      <td>-0.327030</td>\n",
       "      <td>-0.480906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.095498</td>\n",
       "      <td>-0.779528</td>\n",
       "      <td>-0.823690</td>\n",
       "      <td>-0.159587</td>\n",
       "      <td>0.393944</td>\n",
       "      <td>-0.301837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374327</td>\n",
       "      <td>0.085621</td>\n",
       "      <td>-0.115341</td>\n",
       "      <td>0.710098</td>\n",
       "      <td>0.498155</td>\n",
       "      <td>-0.452055</td>\n",
       "      <td>0.672465</td>\n",
       "      <td>-0.756169</td>\n",
       "      <td>-0.096882</td>\n",
       "      <td>-0.567550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090041</td>\n",
       "      <td>-0.811674</td>\n",
       "      <td>-0.868393</td>\n",
       "      <td>0.203260</td>\n",
       "      <td>0.186939</td>\n",
       "      <td>-0.333352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087187</td>\n",
       "      <td>-0.127469</td>\n",
       "      <td>0.075751</td>\n",
       "      <td>0.609228</td>\n",
       "      <td>0.279520</td>\n",
       "      <td>-0.011757</td>\n",
       "      <td>0.636445</td>\n",
       "      <td>-0.375453</td>\n",
       "      <td>-0.311878</td>\n",
       "      <td>-0.438952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042292</td>\n",
       "      <td>-1.075894</td>\n",
       "      <td>-0.557236</td>\n",
       "      <td>0.592413</td>\n",
       "      <td>0.416230</td>\n",
       "      <td>-0.704641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416929</td>\n",
       "      <td>-0.094732</td>\n",
       "      <td>-0.341141</td>\n",
       "      <td>0.928138</td>\n",
       "      <td>0.412233</td>\n",
       "      <td>-0.262490</td>\n",
       "      <td>0.705931</td>\n",
       "      <td>-0.806319</td>\n",
       "      <td>-0.183453</td>\n",
       "      <td>-0.060224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.058663</td>\n",
       "      <td>-1.200765</td>\n",
       "      <td>-0.281757</td>\n",
       "      <td>-0.261214</td>\n",
       "      <td>-0.896719</td>\n",
       "      <td>-0.183152</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.274514</td>\n",
       "      <td>0.602111</td>\n",
       "      <td>-0.016358</td>\n",
       "      <td>0.659440</td>\n",
       "      <td>0.128409</td>\n",
       "      <td>-0.733430</td>\n",
       "      <td>-0.100913</td>\n",
       "      <td>-0.011235</td>\n",
       "      <td>0.463033</td>\n",
       "      <td>0.383730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.040928</td>\n",
       "      <td>-0.974629</td>\n",
       "      <td>0.105890</td>\n",
       "      <td>0.334419</td>\n",
       "      <td>-0.775297</td>\n",
       "      <td>-0.085035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.455192</td>\n",
       "      <td>0.718217</td>\n",
       "      <td>-0.123305</td>\n",
       "      <td>0.863213</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>-0.589624</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>-0.011204</td>\n",
       "      <td>0.159558</td>\n",
       "      <td>0.297565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006821</td>\n",
       "      <td>-0.513226</td>\n",
       "      <td>-0.143244</td>\n",
       "      <td>0.693421</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>-0.162662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252258</td>\n",
       "      <td>-0.838978</td>\n",
       "      <td>-1.282002</td>\n",
       "      <td>1.021543</td>\n",
       "      <td>0.761146</td>\n",
       "      <td>-0.754921</td>\n",
       "      <td>0.750287</td>\n",
       "      <td>-1.533581</td>\n",
       "      <td>0.073937</td>\n",
       "      <td>0.437128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025921</td>\n",
       "      <td>-0.586538</td>\n",
       "      <td>-0.539687</td>\n",
       "      <td>0.035985</td>\n",
       "      <td>0.224866</td>\n",
       "      <td>-0.344206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848082</td>\n",
       "      <td>-0.069036</td>\n",
       "      <td>0.107245</td>\n",
       "      <td>0.903303</td>\n",
       "      <td>0.710448</td>\n",
       "      <td>-0.639074</td>\n",
       "      <td>0.714387</td>\n",
       "      <td>-0.985603</td>\n",
       "      <td>-0.137974</td>\n",
       "      <td>-0.682904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399999 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       province  make  gender       age  tagidLen  tag_emb_0  tag_emb_1  \\\n",
       "0            13     8     1.0  0.000000  0.079127  -0.584356  -0.505823   \n",
       "1            12     1     1.0  0.833333  0.083220  -1.238055  -0.382685   \n",
       "2             2    10     1.0  0.000000  0.065484  -0.207355  -0.698800   \n",
       "3             6     2     1.0  0.333333  0.095498  -0.779528  -0.823690   \n",
       "4            22     2     1.0  0.000000  0.090041  -0.811674  -0.868393   \n",
       "...         ...   ...     ...       ...       ...        ...        ...   \n",
       "99994        16     2     1.0  0.000000  0.042292  -1.075894  -0.557236   \n",
       "99995        10     1     1.0  0.166667  0.058663  -1.200765  -0.281757   \n",
       "99996         9     1     1.0  0.500000  0.040928  -0.974629   0.105890   \n",
       "99997        10     2     1.0  0.000000  0.006821  -0.513226  -0.143244   \n",
       "99998         6     2     1.0  0.000000  0.025921  -0.586538  -0.539687   \n",
       "\n",
       "       tag_emb_2  tag_emb_3  tag_emb_4  ...  tag_emb_118  tag_emb_119  \\\n",
       "0       0.163237  -0.118881  -0.570097  ...    -0.199300    -0.039976   \n",
       "1      -0.172527  -1.336101  -0.141902  ...    -0.149191     0.512553   \n",
       "2      -0.361576  -0.397024  -0.512204  ...    -0.337251    -1.367355   \n",
       "3      -0.159587   0.393944  -0.301837  ...     0.374327     0.085621   \n",
       "4       0.203260   0.186939  -0.333352  ...     0.087187    -0.127469   \n",
       "...          ...        ...        ...  ...          ...          ...   \n",
       "99994   0.592413   0.416230  -0.704641  ...     0.416929    -0.094732   \n",
       "99995  -0.261214  -0.896719  -0.183152  ...    -0.274514     0.602111   \n",
       "99996   0.334419  -0.775297  -0.085035  ...    -0.455192     0.718217   \n",
       "99997   0.693421   0.869403  -0.162662  ...     0.252258    -0.838978   \n",
       "99998   0.035985   0.224866  -0.344206  ...     0.848082    -0.069036   \n",
       "\n",
       "       tag_emb_120  tag_emb_121  tag_emb_122  tag_emb_123  tag_emb_124  \\\n",
       "0         0.120165     0.704987     0.455543    -0.556336     0.340716   \n",
       "1        -0.193722     0.661562    -0.182866    -0.928568    -0.151268   \n",
       "2        -0.110100     0.931193    -0.368334    -0.824169    -0.105975   \n",
       "3        -0.115341     0.710098     0.498155    -0.452055     0.672465   \n",
       "4         0.075751     0.609228     0.279520    -0.011757     0.636445   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "99994    -0.341141     0.928138     0.412233    -0.262490     0.705931   \n",
       "99995    -0.016358     0.659440     0.128409    -0.733430    -0.100913   \n",
       "99996    -0.123305     0.863213     0.032837    -0.589624     0.027972   \n",
       "99997    -1.282002     1.021543     0.761146    -0.754921     0.750287   \n",
       "99998     0.107245     0.903303     0.710448    -0.639074     0.714387   \n",
       "\n",
       "       tag_emb_125  tag_emb_126  tag_emb_127  \n",
       "0        -0.544948    -0.441241    -0.371886  \n",
       "1         0.027404     0.356565     0.194155  \n",
       "2        -0.491890    -0.327030    -0.480906  \n",
       "3        -0.756169    -0.096882    -0.567550  \n",
       "4        -0.375453    -0.311878    -0.438952  \n",
       "...            ...          ...          ...  \n",
       "99994    -0.806319    -0.183453    -0.060224  \n",
       "99995    -0.011235     0.463033     0.383730  \n",
       "99996    -0.011204     0.159558     0.297565  \n",
       "99997    -1.533581     0.073937     0.437128  \n",
       "99998    -0.985603    -0.137974    -0.682904  \n",
       "\n",
       "[399999 rows x 133 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[feature_names].head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "938/938 - 15s - loss: 0.7003 - accuracy: 0.6793 - val_loss: 0.6154 - val_accuracy: 0.6793\n",
      "Epoch 2/10000\n",
      "938/938 - 11s - loss: 0.5966 - accuracy: 0.6908 - val_loss: 0.6020 - val_accuracy: 0.6801\n",
      "Epoch 3/10000\n",
      "938/938 - 11s - loss: 0.5910 - accuracy: 0.6926 - val_loss: 0.5951 - val_accuracy: 0.6844\n",
      "Epoch 4/10000\n",
      "938/938 - 11s - loss: 0.5885 - accuracy: 0.6930 - val_loss: 0.5904 - val_accuracy: 0.6902\n",
      "Epoch 5/10000\n",
      "938/938 - 11s - loss: 0.5861 - accuracy: 0.6950 - val_loss: 0.5884 - val_accuracy: 0.6924\n",
      "Epoch 6/10000\n",
      "938/938 - 11s - loss: 0.5851 - accuracy: 0.6957 - val_loss: 0.5851 - val_accuracy: 0.6947\n",
      "Epoch 7/10000\n",
      "938/938 - 11s - loss: 0.5841 - accuracy: 0.6958 - val_loss: 0.5886 - val_accuracy: 0.6888\n",
      "Epoch 8/10000\n",
      "938/938 - 11s - loss: 0.5843 - accuracy: 0.6947 - val_loss: 0.5896 - val_accuracy: 0.6905\n",
      "Epoch 9/10000\n",
      "938/938 - 11s - loss: 0.5832 - accuracy: 0.6966 - val_loss: 0.5876 - val_accuracy: 0.6910\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 10/10000\n",
      "938/938 - 11s - loss: 0.5778 - accuracy: 0.7003 - val_loss: 0.5818 - val_accuracy: 0.6954\n",
      "Epoch 11/10000\n",
      "938/938 - 11s - loss: 0.5763 - accuracy: 0.7018 - val_loss: 0.5801 - val_accuracy: 0.6967\n",
      "Epoch 12/10000\n",
      "938/938 - 11s - loss: 0.5757 - accuracy: 0.7025 - val_loss: 0.5797 - val_accuracy: 0.6974\n",
      "Epoch 13/10000\n",
      "938/938 - 11s - loss: 0.5753 - accuracy: 0.7019 - val_loss: 0.5803 - val_accuracy: 0.6971\n",
      "Epoch 14/10000\n",
      "938/938 - 11s - loss: 0.5750 - accuracy: 0.7022 - val_loss: 0.5794 - val_accuracy: 0.6974\n",
      "Epoch 15/10000\n",
      "938/938 - 11s - loss: 0.5749 - accuracy: 0.7028 - val_loss: 0.5798 - val_accuracy: 0.6968\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 16/10000\n",
      "938/938 - 11s - loss: 0.5739 - accuracy: 0.7038 - val_loss: 0.5784 - val_accuracy: 0.6983\n",
      "Epoch 17/10000\n",
      "938/938 - 11s - loss: 0.5733 - accuracy: 0.7032 - val_loss: 0.5782 - val_accuracy: 0.6981\n",
      "Epoch 18/10000\n",
      "938/938 - 11s - loss: 0.5734 - accuracy: 0.7038 - val_loss: 0.5783 - val_accuracy: 0.6982\n",
      "Epoch 19/10000\n",
      "938/938 - 11s - loss: 0.5733 - accuracy: 0.7044 - val_loss: 0.5783 - val_accuracy: 0.6984\n",
      "Epoch 20/10000\n",
      "938/938 - 11s - loss: 0.5733 - accuracy: 0.7045 - val_loss: 0.5782 - val_accuracy: 0.6981\n",
      "Epoch 21/10000\n",
      "938/938 - 11s - loss: 0.5729 - accuracy: 0.7042 - val_loss: 0.5781 - val_accuracy: 0.6986\n",
      "Epoch 22/10000\n",
      "938/938 - 11s - loss: 0.5730 - accuracy: 0.7041 - val_loss: 0.5780 - val_accuracy: 0.6982\n",
      "Epoch 23/10000\n",
      "938/938 - 11s - loss: 0.5729 - accuracy: 0.7041 - val_loss: 0.5781 - val_accuracy: 0.6984\n",
      "Epoch 24/10000\n",
      "938/938 - 11s - loss: 0.5729 - accuracy: 0.7047 - val_loss: 0.5780 - val_accuracy: 0.6987\n",
      "Epoch 25/10000\n",
      "938/938 - 11s - loss: 0.5728 - accuracy: 0.7042 - val_loss: 0.5779 - val_accuracy: 0.6983\n",
      "Epoch 26/10000\n",
      "938/938 - 11s - loss: 0.5729 - accuracy: 0.7041 - val_loss: 0.5778 - val_accuracy: 0.6985\n",
      "Epoch 27/10000\n",
      "938/938 - 11s - loss: 0.5729 - accuracy: 0.7039 - val_loss: 0.5778 - val_accuracy: 0.6989\n",
      "Epoch 28/10000\n",
      "938/938 - 11s - loss: 0.5725 - accuracy: 0.7038 - val_loss: 0.5779 - val_accuracy: 0.6986\n",
      "Epoch 29/10000\n",
      "938/938 - 11s - loss: 0.5728 - accuracy: 0.7042 - val_loss: 0.5778 - val_accuracy: 0.6985\n",
      "Epoch 30/10000\n",
      "938/938 - 11s - loss: 0.5724 - accuracy: 0.7040 - val_loss: 0.5776 - val_accuracy: 0.6988\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 31/10000\n",
      "938/938 - 11s - loss: 0.5726 - accuracy: 0.7049 - val_loss: 0.5777 - val_accuracy: 0.6989\n",
      "Epoch 32/10000\n",
      "938/938 - 11s - loss: 0.5729 - accuracy: 0.7039 - val_loss: 0.5777 - val_accuracy: 0.6988\n",
      "Epoch 33/10000\n",
      "938/938 - 11s - loss: 0.5725 - accuracy: 0.7043 - val_loss: 0.5777 - val_accuracy: 0.6987\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 34/10000\n",
      "938/938 - 11s - loss: 0.5726 - accuracy: 0.7047 - val_loss: 0.5777 - val_accuracy: 0.6987\n",
      "Epoch 35/10000\n",
      "938/938 - 11s - loss: 0.5723 - accuracy: 0.7053 - val_loss: 0.5777 - val_accuracy: 0.6987\n",
      "Epoch 36/10000\n",
      "938/938 - 11s - loss: 0.5727 - accuracy: 0.7044 - val_loss: 0.5777 - val_accuracy: 0.6987\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 00036: early stopping\n",
      "fold n2\n",
      "Epoch 1/10000\n",
      "938/938 - 15s - loss: 0.6940 - accuracy: 0.6789 - val_loss: 0.6018 - val_accuracy: 0.6930\n",
      "Epoch 2/10000\n",
      "938/938 - 11s - loss: 0.5991 - accuracy: 0.6896 - val_loss: 0.5883 - val_accuracy: 0.6962\n",
      "Epoch 3/10000\n",
      "938/938 - 11s - loss: 0.5926 - accuracy: 0.6915 - val_loss: 0.5866 - val_accuracy: 0.6974\n",
      "Epoch 4/10000\n",
      "938/938 - 11s - loss: 0.5898 - accuracy: 0.6918 - val_loss: 0.5841 - val_accuracy: 0.6972\n",
      "Epoch 5/10000\n",
      "938/938 - 11s - loss: 0.5884 - accuracy: 0.6926 - val_loss: 0.5841 - val_accuracy: 0.6958\n",
      "Epoch 6/10000\n",
      "938/938 - 11s - loss: 0.5877 - accuracy: 0.6925 - val_loss: 0.5843 - val_accuracy: 0.6954\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/10000\n",
      "938/938 - 11s - loss: 0.5818 - accuracy: 0.6977 - val_loss: 0.5771 - val_accuracy: 0.7032\n",
      "Epoch 8/10000\n",
      "938/938 - 11s - loss: 0.5800 - accuracy: 0.6987 - val_loss: 0.5760 - val_accuracy: 0.7030\n",
      "Epoch 9/10000\n",
      "938/938 - 11s - loss: 0.5790 - accuracy: 0.6996 - val_loss: 0.5749 - val_accuracy: 0.7033\n",
      "Epoch 10/10000\n",
      "938/938 - 11s - loss: 0.5781 - accuracy: 0.7000 - val_loss: 0.5741 - val_accuracy: 0.7039\n",
      "Epoch 11/10000\n",
      "938/938 - 11s - loss: 0.5781 - accuracy: 0.7003 - val_loss: 0.5738 - val_accuracy: 0.7046\n",
      "Epoch 12/10000\n",
      "938/938 - 11s - loss: 0.5773 - accuracy: 0.7013 - val_loss: 0.5738 - val_accuracy: 0.7036\n",
      "Epoch 13/10000\n",
      "938/938 - 11s - loss: 0.5772 - accuracy: 0.7011 - val_loss: 0.5726 - val_accuracy: 0.7054\n",
      "Epoch 14/10000\n",
      "938/938 - 11s - loss: 0.5765 - accuracy: 0.7012 - val_loss: 0.5731 - val_accuracy: 0.7053\n",
      "Epoch 15/10000\n",
      "938/938 - 11s - loss: 0.5763 - accuracy: 0.7027 - val_loss: 0.5725 - val_accuracy: 0.7052\n",
      "Epoch 16/10000\n",
      "938/938 - 11s - loss: 0.5759 - accuracy: 0.7010 - val_loss: 0.5730 - val_accuracy: 0.7042\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 17/10000\n",
      "938/938 - 11s - loss: 0.5752 - accuracy: 0.7018 - val_loss: 0.5714 - val_accuracy: 0.7067\n",
      "Epoch 18/10000\n",
      "938/938 - 11s - loss: 0.5745 - accuracy: 0.7030 - val_loss: 0.5712 - val_accuracy: 0.7066\n",
      "Epoch 19/10000\n",
      "938/938 - 11s - loss: 0.5743 - accuracy: 0.7032 - val_loss: 0.5712 - val_accuracy: 0.7064\n",
      "Epoch 20/10000\n",
      "938/938 - 11s - loss: 0.5742 - accuracy: 0.7032 - val_loss: 0.5711 - val_accuracy: 0.7062\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 21/10000\n",
      "938/938 - 11s - loss: 0.5740 - accuracy: 0.7030 - val_loss: 0.5711 - val_accuracy: 0.7061\n",
      "Epoch 22/10000\n",
      "938/938 - 11s - loss: 0.5742 - accuracy: 0.7036 - val_loss: 0.5711 - val_accuracy: 0.7063\n",
      "Epoch 00022: early stopping\n",
      "fold n3\n",
      "Epoch 1/10000\n",
      "938/938 - 15s - loss: 0.6966 - accuracy: 0.6767 - val_loss: 0.5996 - val_accuracy: 0.6923\n",
      "Epoch 2/10000\n",
      "938/938 - 11s - loss: 0.5982 - accuracy: 0.6892 - val_loss: 0.5930 - val_accuracy: 0.6920\n",
      "Epoch 3/10000\n",
      "938/938 - 11s - loss: 0.5919 - accuracy: 0.6911 - val_loss: 0.5845 - val_accuracy: 0.6980\n",
      "Epoch 4/10000\n",
      "938/938 - 11s - loss: 0.5891 - accuracy: 0.6925 - val_loss: 0.5838 - val_accuracy: 0.6984\n",
      "Epoch 5/10000\n",
      "938/938 - 11s - loss: 0.5878 - accuracy: 0.6923 - val_loss: 0.5823 - val_accuracy: 0.6980\n",
      "Epoch 6/10000\n",
      "938/938 - 12s - loss: 0.5872 - accuracy: 0.6930 - val_loss: 0.5819 - val_accuracy: 0.6976\n",
      "Epoch 7/10000\n",
      "938/938 - 11s - loss: 0.5860 - accuracy: 0.6934 - val_loss: 0.5808 - val_accuracy: 0.6986\n",
      "Epoch 8/10000\n",
      "938/938 - 11s - loss: 0.5855 - accuracy: 0.6938 - val_loss: 0.5806 - val_accuracy: 0.6974\n",
      "Epoch 9/10000\n",
      "938/938 - 11s - loss: 0.5845 - accuracy: 0.6950 - val_loss: 0.5802 - val_accuracy: 0.6974\n",
      "Epoch 10/10000\n",
      "938/938 - 11s - loss: 0.5844 - accuracy: 0.6941 - val_loss: 0.5801 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/10000\n",
      "938/938 - 11s - loss: 0.5793 - accuracy: 0.6993 - val_loss: 0.5762 - val_accuracy: 0.7009\n",
      "Epoch 12/10000\n",
      "938/938 - 11s - loss: 0.5778 - accuracy: 0.6996 - val_loss: 0.5752 - val_accuracy: 0.7018\n",
      "Epoch 13/10000\n",
      "938/938 - 12s - loss: 0.5772 - accuracy: 0.7014 - val_loss: 0.5746 - val_accuracy: 0.7022\n",
      "Epoch 14/10000\n",
      "938/938 - 11s - loss: 0.5767 - accuracy: 0.7008 - val_loss: 0.5743 - val_accuracy: 0.7017\n",
      "Epoch 15/10000\n",
      "938/938 - 11s - loss: 0.5763 - accuracy: 0.7007 - val_loss: 0.5738 - val_accuracy: 0.7020\n",
      "Epoch 16/10000\n",
      "938/938 - 11s - loss: 0.5756 - accuracy: 0.7013 - val_loss: 0.5732 - val_accuracy: 0.7024\n",
      "Epoch 17/10000\n",
      "938/938 - 11s - loss: 0.5758 - accuracy: 0.7008 - val_loss: 0.5733 - val_accuracy: 0.7026\n",
      "Epoch 18/10000\n",
      "938/938 - 11s - loss: 0.5752 - accuracy: 0.7020 - val_loss: 0.5727 - val_accuracy: 0.7035\n",
      "Epoch 19/10000\n",
      "938/938 - 11s - loss: 0.5751 - accuracy: 0.7024 - val_loss: 0.5730 - val_accuracy: 0.7023\n",
      "Epoch 20/10000\n",
      "938/938 - 11s - loss: 0.5749 - accuracy: 0.7027 - val_loss: 0.5728 - val_accuracy: 0.7032\n",
      "Epoch 21/10000\n",
      "938/938 - 11s - loss: 0.5745 - accuracy: 0.7021 - val_loss: 0.5723 - val_accuracy: 0.7032\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 22/10000\n",
      "938/938 - 11s - loss: 0.5734 - accuracy: 0.7031 - val_loss: 0.5718 - val_accuracy: 0.7038\n",
      "Epoch 23/10000\n",
      "938/938 - 11s - loss: 0.5734 - accuracy: 0.7032 - val_loss: 0.5717 - val_accuracy: 0.7042\n",
      "Epoch 24/10000\n",
      "938/938 - 11s - loss: 0.5732 - accuracy: 0.7030 - val_loss: 0.5716 - val_accuracy: 0.7038\n",
      "Epoch 25/10000\n",
      "938/938 - 11s - loss: 0.5731 - accuracy: 0.7037 - val_loss: 0.5717 - val_accuracy: 0.7036\n",
      "Epoch 26/10000\n",
      "938/938 - 11s - loss: 0.5731 - accuracy: 0.7033 - val_loss: 0.5716 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 27/10000\n",
      "938/938 - 11s - loss: 0.5730 - accuracy: 0.7040 - val_loss: 0.5716 - val_accuracy: 0.7041\n",
      "Epoch 28/10000\n",
      "938/938 - 12s - loss: 0.5731 - accuracy: 0.7039 - val_loss: 0.5716 - val_accuracy: 0.7041\n",
      "Epoch 00028: early stopping\n",
      "fold n4\n",
      "Epoch 1/10000\n",
      "938/938 - 15s - loss: 0.6945 - accuracy: 0.6777 - val_loss: 0.6010 - val_accuracy: 0.6918\n",
      "Epoch 2/10000\n",
      "938/938 - 11s - loss: 0.5981 - accuracy: 0.6899 - val_loss: 0.5910 - val_accuracy: 0.6928\n",
      "Epoch 3/10000\n",
      "938/938 - 12s - loss: 0.5919 - accuracy: 0.6912 - val_loss: 0.5934 - val_accuracy: 0.6869\n",
      "Epoch 4/10000\n",
      "938/938 - 12s - loss: 0.5891 - accuracy: 0.6929 - val_loss: 0.5862 - val_accuracy: 0.6947\n",
      "Epoch 5/10000\n",
      "938/938 - 11s - loss: 0.5872 - accuracy: 0.6937 - val_loss: 0.5845 - val_accuracy: 0.6948\n",
      "Epoch 6/10000\n",
      "938/938 - 11s - loss: 0.5866 - accuracy: 0.6941 - val_loss: 0.5832 - val_accuracy: 0.6978\n",
      "Epoch 7/10000\n",
      "938/938 - 11s - loss: 0.5855 - accuracy: 0.6945 - val_loss: 0.5834 - val_accuracy: 0.6957\n",
      "Epoch 8/10000\n",
      "938/938 - 11s - loss: 0.5846 - accuracy: 0.6948 - val_loss: 0.5843 - val_accuracy: 0.6963\n",
      "Epoch 9/10000\n",
      "938/938 - 11s - loss: 0.5839 - accuracy: 0.6954 - val_loss: 0.5804 - val_accuracy: 0.6989\n",
      "Epoch 10/10000\n",
      "938/938 - 11s - loss: 0.5835 - accuracy: 0.6954 - val_loss: 0.5839 - val_accuracy: 0.6954\n",
      "Epoch 11/10000\n",
      "938/938 - 11s - loss: 0.5836 - accuracy: 0.6949 - val_loss: 0.5794 - val_accuracy: 0.7013\n",
      "Epoch 12/10000\n",
      "938/938 - 11s - loss: 0.5831 - accuracy: 0.6959 - val_loss: 0.5802 - val_accuracy: 0.6985\n",
      "Epoch 13/10000\n",
      "938/938 - 11s - loss: 0.5832 - accuracy: 0.6954 - val_loss: 0.5803 - val_accuracy: 0.6987\n",
      "Epoch 14/10000\n",
      "938/938 - 12s - loss: 0.5827 - accuracy: 0.6954 - val_loss: 0.5811 - val_accuracy: 0.6980\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 15/10000\n",
      "938/938 - 12s - loss: 0.5779 - accuracy: 0.7003 - val_loss: 0.5762 - val_accuracy: 0.7024\n",
      "Epoch 16/10000\n",
      "938/938 - 12s - loss: 0.5763 - accuracy: 0.7012 - val_loss: 0.5754 - val_accuracy: 0.7035\n",
      "Epoch 17/10000\n",
      "938/938 - 11s - loss: 0.5757 - accuracy: 0.7013 - val_loss: 0.5748 - val_accuracy: 0.7042\n",
      "Epoch 18/10000\n",
      "938/938 - 12s - loss: 0.5752 - accuracy: 0.7014 - val_loss: 0.5744 - val_accuracy: 0.7043\n",
      "Epoch 19/10000\n",
      "938/938 - 11s - loss: 0.5751 - accuracy: 0.7017 - val_loss: 0.5750 - val_accuracy: 0.7033\n",
      "Epoch 20/10000\n",
      "938/938 - 11s - loss: 0.5743 - accuracy: 0.7017 - val_loss: 0.5741 - val_accuracy: 0.7049\n",
      "Epoch 21/10000\n",
      "938/938 - 11s - loss: 0.5745 - accuracy: 0.7020 - val_loss: 0.5738 - val_accuracy: 0.7047\n",
      "Epoch 22/10000\n",
      "938/938 - 11s - loss: 0.5743 - accuracy: 0.7021 - val_loss: 0.5744 - val_accuracy: 0.7043\n",
      "Epoch 23/10000\n",
      "938/938 - 12s - loss: 0.5739 - accuracy: 0.7026 - val_loss: 0.5737 - val_accuracy: 0.7046\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 24/10000\n",
      "938/938 - 11s - loss: 0.5731 - accuracy: 0.7033 - val_loss: 0.5728 - val_accuracy: 0.7053\n",
      "Epoch 25/10000\n",
      "938/938 - 11s - loss: 0.5727 - accuracy: 0.7035 - val_loss: 0.5728 - val_accuracy: 0.7057\n",
      "Epoch 26/10000\n",
      "938/938 - 11s - loss: 0.5730 - accuracy: 0.7032 - val_loss: 0.5728 - val_accuracy: 0.7049\n",
      "Epoch 27/10000\n",
      "938/938 - 11s - loss: 0.5727 - accuracy: 0.7035 - val_loss: 0.5726 - val_accuracy: 0.7052\n",
      "Epoch 28/10000\n",
      "938/938 - 11s - loss: 0.5726 - accuracy: 0.7028 - val_loss: 0.5726 - val_accuracy: 0.7058\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 29/10000\n",
      "938/938 - 11s - loss: 0.5726 - accuracy: 0.7047 - val_loss: 0.5726 - val_accuracy: 0.7053\n",
      "Epoch 30/10000\n",
      "938/938 - 11s - loss: 0.5726 - accuracy: 0.7042 - val_loss: 0.5726 - val_accuracy: 0.7054\n",
      "Epoch 31/10000\n",
      "938/938 - 11s - loss: 0.5725 - accuracy: 0.7034 - val_loss: 0.5726 - val_accuracy: 0.7053\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 32/10000\n",
      "938/938 - 11s - loss: 0.5727 - accuracy: 0.7036 - val_loss: 0.5726 - val_accuracy: 0.7053\n",
      "Epoch 33/10000\n",
      "938/938 - 11s - loss: 0.5726 - accuracy: 0.7043 - val_loss: 0.5726 - val_accuracy: 0.7053\n",
      "Epoch 00033: early stopping\n",
      "fold n5\n",
      "Epoch 1/10000\n",
      "938/938 - 16s - loss: 0.6984 - accuracy: 0.6780 - val_loss: 0.6021 - val_accuracy: 0.6919\n",
      "Epoch 2/10000\n",
      "938/938 - 11s - loss: 0.5987 - accuracy: 0.6896 - val_loss: 0.5890 - val_accuracy: 0.6946\n",
      "Epoch 3/10000\n",
      "938/938 - 11s - loss: 0.5924 - accuracy: 0.6907 - val_loss: 0.5858 - val_accuracy: 0.6941\n",
      "Epoch 4/10000\n",
      "938/938 - 11s - loss: 0.5895 - accuracy: 0.6924 - val_loss: 0.5839 - val_accuracy: 0.6978\n",
      "Epoch 5/10000\n",
      "938/938 - 11s - loss: 0.5884 - accuracy: 0.6926 - val_loss: 0.5863 - val_accuracy: 0.6917\n",
      "Epoch 6/10000\n",
      "938/938 - 11s - loss: 0.5873 - accuracy: 0.6931 - val_loss: 0.5848 - val_accuracy: 0.6934\n",
      "Epoch 7/10000\n",
      "938/938 - 11s - loss: 0.5868 - accuracy: 0.6940 - val_loss: 0.5818 - val_accuracy: 0.6969\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/10000\n",
      "938/938 - 11s - loss: 0.5810 - accuracy: 0.6990 - val_loss: 0.5767 - val_accuracy: 0.7003\n",
      "Epoch 9/10000\n",
      "938/938 - 11s - loss: 0.5793 - accuracy: 0.6997 - val_loss: 0.5759 - val_accuracy: 0.6998\n",
      "Epoch 10/10000\n",
      "938/938 - 11s - loss: 0.5783 - accuracy: 0.7003 - val_loss: 0.5748 - val_accuracy: 0.7014\n",
      "Epoch 11/10000\n",
      "938/938 - 11s - loss: 0.5775 - accuracy: 0.7014 - val_loss: 0.5745 - val_accuracy: 0.7010\n",
      "Epoch 12/10000\n",
      "938/938 - 12s - loss: 0.5773 - accuracy: 0.7017 - val_loss: 0.5744 - val_accuracy: 0.7009\n",
      "Epoch 13/10000\n",
      "938/938 - 11s - loss: 0.5766 - accuracy: 0.7014 - val_loss: 0.5736 - val_accuracy: 0.7017\n",
      "Epoch 14/10000\n",
      "938/938 - 11s - loss: 0.5761 - accuracy: 0.7011 - val_loss: 0.5730 - val_accuracy: 0.7027\n",
      "Epoch 15/10000\n",
      "938/938 - 11s - loss: 0.5758 - accuracy: 0.7022 - val_loss: 0.5727 - val_accuracy: 0.7026\n",
      "Epoch 16/10000\n",
      "938/938 - 11s - loss: 0.5756 - accuracy: 0.7016 - val_loss: 0.5724 - val_accuracy: 0.7033\n",
      "Epoch 17/10000\n",
      "938/938 - 11s - loss: 0.5753 - accuracy: 0.7029 - val_loss: 0.5726 - val_accuracy: 0.7023\n",
      "Epoch 18/10000\n",
      "938/938 - 11s - loss: 0.5748 - accuracy: 0.7022 - val_loss: 0.5719 - val_accuracy: 0.7043\n",
      "Epoch 19/10000\n",
      "938/938 - 11s - loss: 0.5747 - accuracy: 0.7034 - val_loss: 0.5719 - val_accuracy: 0.7033\n",
      "Epoch 20/10000\n",
      "938/938 - 11s - loss: 0.5745 - accuracy: 0.7037 - val_loss: 0.5714 - val_accuracy: 0.7041\n",
      "Epoch 21/10000\n",
      "938/938 - 11s - loss: 0.5742 - accuracy: 0.7030 - val_loss: 0.5710 - val_accuracy: 0.7039\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 22/10000\n",
      "938/938 - 11s - loss: 0.5730 - accuracy: 0.7041 - val_loss: 0.5706 - val_accuracy: 0.7050\n",
      "Epoch 23/10000\n",
      "938/938 - 11s - loss: 0.5722 - accuracy: 0.7049 - val_loss: 0.5705 - val_accuracy: 0.7049\n",
      "Epoch 24/10000\n",
      "938/938 - 11s - loss: 0.5728 - accuracy: 0.7045 - val_loss: 0.5704 - val_accuracy: 0.7049\n",
      "Epoch 25/10000\n",
      "938/938 - 11s - loss: 0.5726 - accuracy: 0.7037 - val_loss: 0.5705 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 26/10000\n",
      "938/938 - 11s - loss: 0.5725 - accuracy: 0.7049 - val_loss: 0.5703 - val_accuracy: 0.7051\n",
      "Epoch 27/10000\n",
      "938/938 - 11s - loss: 0.5726 - accuracy: 0.7048 - val_loss: 0.5703 - val_accuracy: 0.7049\n",
      "Epoch 28/10000\n",
      "938/938 - 11s - loss: 0.5725 - accuracy: 0.7050 - val_loss: 0.5703 - val_accuracy: 0.7049\n",
      "Epoch 29/10000\n",
      "938/938 - 11s - loss: 0.5722 - accuracy: 0.7048 - val_loss: 0.5703 - val_accuracy: 0.7050\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 30/10000\n",
      "938/938 - 11s - loss: 0.5724 - accuracy: 0.7047 - val_loss: 0.5703 - val_accuracy: 0.7049\n",
      "Epoch 31/10000\n",
      "938/938 - 11s - loss: 0.5727 - accuracy: 0.7042 - val_loss: 0.5703 - val_accuracy: 0.7049\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [300000, 60000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f7906c23dd23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val LogLoss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val AUC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2240\u001b[0m     \"\"\"\n\u001b[1;32m   2241\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [300000, 60000]"
     ]
    }
   ],
   "source": [
    "# # 3.generate input data for model\n",
    "newtrain = data[data['label']!= 2]\n",
    "test = data[data['label']==2]\n",
    "test_model_input = {name: test[name] for name in feature_names}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2048)\n",
    "oof = np.zeros([len(newtrain), 1])\n",
    "predictions = np.zeros([len(test), 1])\n",
    "\n",
    "for i, (trn_idx, val_idx) in enumerate(folds.split(newtrain, newtrain['label'])):\n",
    "    print(\"fold n{}\".format(i + 1))\n",
    "    tr_x = newtrain.iloc[trn_idx,:]\n",
    "    tr_y = newtrain.iloc[trn_idx]['label']\n",
    "    val_x = newtrain.iloc[val_idx,:]\n",
    "    val_y = newtrain.iloc[val_idx]['label']\n",
    "    train_model_input = {name: tr_x[name] for name in feature_names}  #每个特征都是一个input\n",
    "    val_model_input = {name: val_x[name] for name in feature_names}\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "    # model = NFFM(linear_feature_columns, dnn_feature_columns, l2_reg_dnn=0.01, dnn_dropout=0.5, task='binary')\n",
    "    #model = FGCNN(dnn_feature_columns,l2_reg_dnn=0.01,dnn_dropout=0.5,task='binary')\n",
    "    model = DeepFM(linear_feature_columns, dnn_feature_columns,dnn_hidden_units=[256,256,128], l2_reg_dnn=0.001, dnn_dropout=0.25, task='binary')\n",
    "\n",
    "    #if fold_ == 0:\n",
    "    #model.summary()\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=Adam(lr=0.001),metrics=['accuracy'], )\n",
    "   \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=3, mode='auto',min_delta=0.0001, verbose=2)  #改监控指标为val_acc\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5,verbose=2)\n",
    "    bst_model_path = \"./model/deepfm_{}.h5\".format(1)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    history = model.fit(train_model_input, tr_y,\n",
    "                    batch_size=256, epochs=10000, verbose=2, validation_data=(val_model_input,val_y),callbacks=[early_stopping, model_checkpoint,reduce_lr])\n",
    "\n",
    "    model.load_weights(bst_model_path)    \n",
    "    oof[val_idx] = model.predict(val_model_input, batch_size=256)\n",
    "    predictions += model.predict(test_model_input, batch_size=256)\n",
    "print(\"val LogLoss\", round(log_loss(newtrain['label'].values, oof), 4))\n",
    "print(\"val AUC\", round(roc_auc_score(newtrain['label'].values, oof), 4))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val LogLoss 0.5654\n",
      "val AUC 0.7771\n"
     ]
    }
   ],
   "source": [
    "print(\"val LogLoss\", round(log_loss(newtrain['label'].values, oof), 4))\n",
    "print(\"val AUC\", round(roc_auc_score(newtrain['label'].values, oof), 4))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model,to_file=\"model.png\",show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "submit = oritest[['pid']]\n",
    "submit['proba'] = predictions\n",
    "submit.columns = ['user_id', 'proba']\n",
    "\n",
    "submit = submit.sort_values(['proba'],ascending=False)\n",
    "submit = submit.reset_index(drop=True)\n",
    "submit['category_id'] = 1\n",
    "submit.loc[int(len(submit)*0.5):,'category_id'] = 0\n",
    "\n",
    "submit[['user_id', 'proba']].to_csv('./sub/sub_proba_deepfm.csv', index=False)\n",
    "submit[['user_id', 'category_id']].to_csv('./sub/sub_deepfm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
